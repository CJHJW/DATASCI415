optimal_degree
## (d)
library(splines)
model_spline <- lm(nox ~ bs(dis, df = 4), data = Boston)
# Summary of the model
summary(model_spline)
plot(Boston$dis, Boston$nox, col = "gray", pch = 20,
main = "Regression Spline Fit (df = 4)", xlab = "dis", ylab = "nox")
dis_grid <- seq(min(Boston$dis), max(Boston$dis), length.out = 300)
pred_spline <- predict(model_spline, newdata = data.frame(dis = dis_grid))
lines(dis_grid, pred_spline, col = "blue", lwd = 2)
## (e)
x <- Boston$dis
y <- Boston$nox
rss_values <- numeric(8)
dfs <- 3:10
plot(x, y, col = "gray", pch = 20,
main = "Regression Spline Fits (df = 3 to 10)",
xlab = "dis", ylab = "nox")
x_grid <- seq(min(x), max(x), length.out = 300)
colors <- rainbow(length(dfs))
for (i in seq_along(dfs)) {
df_val <- dfs[i]
model <- lm(nox ~ bs(dis, df = df_val), data = Boston)
rss_values[i] <- sum(residuals(model)^2)
y_pred <- predict(model, newdata = data.frame(dis = x_grid))
lines(x_grid, y_pred, col = colors[i], lwd = 1.5)
}
legend("topright", legend = paste("df =", dfs), col = colors, lwd = 3, cex = 0.8)
## (f)
dfs <- 3:10
cv_errors <- numeric(length(dfs))
for (i in seq_along(dfs)) {
df_val <- dfs[i]
model <- glm(nox ~ bs(dis, df = df_val), data = Boston)
cv_result <- cv.glm(Boston, model, K = 10)
cv_errors[i] <- cv_result$delta[1]
}
best_df <- dfs[which.min(cv_errors)]
best_df
library(ISLR2)
data("Weekly")
head(Weekly)
## (a)
model_a <- glm(Direction ~ Lag1 + Lag2, data = Weekly, family = binomial)
summary(model_a)
## (b)
model_b <- glm(Direction ~ Lag1 + Lag2, data = Weekly[-1, ], family = binomial)
summary(model_b)
## (c)
first <- Weekly[1, ]
prob <- predict(model_b, newdata = first, type = "response")
prob
n <- nrow(Weekly)
errors <- rep(NA, n)
for (i in 1:n) {
train_data <- Weekly[-i, ]
model <- glm(Direction ~ Lag1 + Lag2, data = train_data, family = binomial)
prob <- predict(model, newdata = Weekly[i, ], type = "response")
pred <- ifelse(prob > 0.5, "Up", "Down")
actual <- as.character(Smarket$Direction[i])
errors[i] <- ifelse(pred != actual, 1, 0)
}
n <- nrow(Weekly)
errors <- rep(NA, n)
for (i in 1:n) {
train_data <- Weekly[-i, ]
model <- glm(Direction ~ Lag1 + Lag2, data = train_data, family = binomial)
prob <- predict(model, newdata = Weekly[i, ], type = "response")
pred <- ifelse(prob > 0.5, "Up", "Down")
actual <- as.character(Smarket$Direction[i])
errors[i] <- ifelse(pred != actual, 1, 0)
}
n <- nrow(Weekly)
## (d)
n <- nrow(Weekly)
errors <- rep(NA, n)
for (i in 1:n) {
train_data <- Weekly[-i, ]
model <- glm(Direction ~ Lag1 + Lag2, data = train_data, family = binomial)
prob <- predict(model, newdata = Weekly[i, ], type = "response")
pred <- ifelse(prob > 0.5, "Up", "Down")
actual <- as.character(Smarket$Direction[i])
errors[i] <- ifelse(pred != actual, 1, 0)
}
## (e)
error_rate <- mean(errors)
## (e)
error_rate <- mean(errors)
print(paste("LOOCV Error Rate:", round(error_rate, 4)))
# Problem 2
set.seed(123)
## (e)
error_rate <- mean(errors)
print(paste("LOOCV Error Rate:", round(error_rate, 3)))
# Problem 2
set.seed(123)
data("Boston")
head(Boston[, c('dis', 'nox')])
model_poly3 <- lm(nox ~ poly(dis, 3), data = Boston)
summary(model_poly3)
plot(Boston$dis, Boston$nox, main = "Cubic Polynomial: nox vs dis",
xlab = "dis", ylab = "nox", pch = 18, col = "red")
dis_values <- seq(min(Boston$dis), max(Boston$dis), length.out = 100)
predicted_nox <- predict(model_poly3, newdata = data.frame(dis = dis_values))
lines(dis_values, predicted_nox, col = "blue", lwd = 2)
## (b)
x <- Boston$dis
y <- Boston$nox
rss <- numeric(10)
for (d in 1:10) {
model <- lm(nox ~ poly(dis, d), data = Boston)
rss[d] <- sum(residuals(model)^2)
}
plot(x, y, main = "Polynomial Degrees 1 to 10",
xlab = "dis", ylab = "nox", col = "grey", pch = 20)
dis_grid <- seq(min(x), max(x), length.out = 300)
colors <- rainbow(10)
for (d in 1:10) {
model <- lm(nox ~ poly(dis, d), data = Boston)
pred <- predict(model, newdata = data.frame(dis = dis_grid))
lines(dis_grid, pred, col = colors[d], lwd = 1.5)
}
legend("topright", legend = paste("Degree", 1:10), col = colors, lwd = 5, cex = 0.8)
plot(1:10, rss_values, type = "b", pch = 19,
xlab = "Polynomial Degree", ylab = "Residual Sum of Squares (RSS)",
main = "RSS vs Polynomial Degree")
plot(1:10, rss_values, type = "b", pch = 19,
xlab = "Polynomial Degree", ylab = "Residual Sum of Squares (RSS)",
main = "RSS vs Polynomial Degree")
## (b)
x <- Boston$dis
y <- Boston$nox
rss <- numeric(10)
for (d in 1:10) {
model <- lm(nox ~ poly(dis, d), data = Boston)
rss[d] <- sum(residuals(model)^2)
}
plot(1:10, rss_values, type = "b", pch = 19,
xlab = "Polynomial Degree", ylab = "Residual Sum of Squares (RSS)",
main = "RSS vs Polynomial Degree")
plot(1:10, rss, type = "b", pch = 19,
xlab = "Polynomial Degree", ylab = "Residual Sum of Squares (RSS)",
main = "RSS vs Polynomial Degree")
## (b)
x <- Boston$dis
y <- Boston$nox
rss <- numeric(10)
for (d in 1:10) {
model <- lm(nox ~ poly(dis, d), data = Boston)
rss[d] <- sum(residuals(model)^2)
}
plot(1:10, rss, type = "b", pch = 19,
xlab = "Polynomial Degree", ylab = "Residual Sum of Squares (RSS)",
main = "RSS vs Polynomial Degree")
plot(x, y, main = "Polynomial Degrees 1 to 10",
xlab = "dis", ylab = "nox", col = "grey", pch = 20)
dis_grid <- seq(min(x), max(x), length.out = 300)
colors <- rainbow(10)
for (d in 1:10) {
model <- lm(nox ~ poly(dis, d), data = Boston)
pred <- predict(model, newdata = data.frame(dis = dis_grid))
lines(dis_grid, pred, col = colors[d], lwd = 1.5)
}
legend("topright", legend = paste("Degree", 1:10), col = colors, lwd = 5, cex = 0.8)
## (b)
x <- Boston$dis
y <- Boston$nox
rss <- numeric(10)
for (d in 1:10) {
model <- lm(nox ~ poly(dis, d), data = Boston)
rss[d] <- sum(residuals(model)^2)
}
plot(1:10, rss, type = "b", pch = 19,
xlab = "Polynomial Degree", ylab = "RSS",
main = "RSS vs Polynomial Degree")
plot(x, y, main = "Polynomial Degrees 1 to 10",
xlab = "dis", ylab = "nox", col = "grey", pch = 20)
dis_grid <- seq(min(x), max(x), length.out = 300)
colors <- rainbow(10)
for (d in 1:10) {
model <- lm(nox ~ poly(dis, d), data = Boston)
pred <- predict(model, newdata = data.frame(dis = dis_grid))
lines(dis_grid, pred, col = colors[d], lwd = 1.5)
}
legend("topright", legend = paste("Degree", 1:10), col = colors, lwd = 5, cex = 0.8)
## (c)
library(boot)
cv_error <- numeric(10)
for (d in 1:10) {
model <- glm(nox ~ poly(dis, d), data = Boston)
cv_result <- cv.glm(Boston, model, K = 10)
cv_error[d] <- cv_result$delta[1]
print(cv_error[d])
}
optimal_degree <- which.min(cv_error)
optimal_degree
## (d)
library(splines)
model_spline <- lm(nox ~ bs(dis, df = 4), data = Boston)
# Summary of the model
summary(model_spline)
plot(Boston$dis, Boston$nox, col = "gray", pch = 20,
main = "Regression Spline Fit (df = 4)", xlab = "dis", ylab = "nox")
dis_grid <- seq(min(Boston$dis), max(Boston$dis), length.out = 300)
pred_spline <- predict(model_spline, newdata = data.frame(dis = dis_grid))
lines(dis_grid, pred_spline, col = "blue", lwd = 2)
## (e)
x <- Boston$dis
y <- Boston$nox
rss_values <- numeric(8)
dfs <- 3:10
plot(x, y, col = "gray", pch = 20,
main = "Regression Spline Fits (df = 3 to 10)",
xlab = "dis", ylab = "nox")
x_grid <- seq(min(x), max(x), length.out = 300)
colors <- rainbow(length(dfs))
for (i in seq_along(dfs)) {
df_val <- dfs[i]
model <- lm(nox ~ bs(dis, df = df_val), data = Boston)
rss_values[i] <- sum(residuals(model)^2)
y_pred <- predict(model, newdata = data.frame(dis = x_grid))
lines(x_grid, y_pred, col = colors[i], lwd = 1.5)
}
legend("topright", legend = paste("df =", dfs), col = colors, lwd = 3, cex = 0.8)
## (e)
x <- Boston$dis
y <- Boston$nox
rss_values <- numeric(8)
dfs <- 3:10
plot(1:10, rss, type = "b", pch = 19,
xlab = "Polynomial Degree", ylab = "RSS",
main = "RSS vs Polynomial Degree")
## (e)
x <- Boston$dis
y <- Boston$nox
rss_values <- numeric(8)
dfs <- 3:10
plot(x, y, col = "gray", pch = 20,
main = "Regression Spline Fits (df = 3 to 10)",
xlab = "dis", ylab = "nox")
x_grid <- seq(min(x), max(x), length.out = 300)
colors <- rainbow(length(dfs))
for (i in seq_along(dfs)) {
df_val <- dfs[i]
model <- lm(nox ~ bs(dis, df = df_val), data = Boston)
rss_values[i] <- sum(residuals(model)^2)
y_pred <- predict(model, newdata = data.frame(dis = x_grid))
lines(x_grid, y_pred, col = colors[i], lwd = 1.5)
}
legend("topright", legend = paste("df =", dfs), col = colors, lwd = 3, cex = 0.8)
plot(1:10, rss_values, type = "b", pch = 19,
xlab = "Spline Df", ylab = "RSS",
main = "RSS vs Spline Df")
## (f)
dfs <- 3:10
cv_errors <- numeric(length(dfs))
plot(3:10, rss_values, type = "b", pch = 19,
xlab = "Spline Df", ylab = "RSS",
main = "RSS vs Spline Df")
## (e)
x <- Boston$dis
y <- Boston$nox
rss_values <- numeric(8)
dfs <- 3:10
plot(x, y, col = "gray", pch = 20,
main = "Regression Spline Fits (df = 3 to 10)",
xlab = "dis", ylab = "nox")
x_grid <- seq(min(x), max(x), length.out = 300)
colors <- rainbow(length(dfs))
for (i in seq_along(dfs)) {
df_val <- dfs[i]
model <- lm(nox ~ bs(dis, df = df_val), data = Boston)
rss_values[i] <- sum(residuals(model)^2)
y_pred <- predict(model, newdata = data.frame(dis = x_grid))
lines(x_grid, y_pred, col = colors[i], lwd = 1.5)
}
legend("topright", legend = paste("df =", dfs), col = colors, lwd = 3, cex = 0.8)
plot(3:10, rss_values, type = "b", pch = 19,
xlab = "Spline Df", ylab = "RSS",
main = "RSS vs Spline Df")
## (f)
dfs <- 3:10
cv_errors <- numeric(length(dfs))
for (i in seq_along(dfs)) {
df_val <- dfs[i]
model <- glm(nox ~ bs(dis, df = df_val), data = Boston)
cv_result <- cv.glm(Boston, model, K = 10)
cv_errors[i] <- cv_result$delta[1]
}
best_df <- dfs[which.min(cv_errors)]
best_df
## (f)
dfs <- 3:10
cv_errors <- numeric(length(dfs))
for (i in seq_along(dfs)) {
df_val <- dfs[i]
model <- glm(nox ~ bs(dis, df = df_val), data = Boston)
cv_result <- cv.glm(Boston, model, K = 10)
cv_errors[i] <- cv_result$delta[1]
print(cv_errors[i])
}
best_df <- dfs[which.min(cv_errors)]
best_df
## (f)
library(boot)
set.seed(123)
cv.error_df <- rep(3, 10)
for(i in 3:10) {
df_val <- i
fit <- glm(nox ~ bs(dis, df = df_val), data = Boston)
cv.error_df[i] <- cv.glm(Boston, fit, K = 10)$delta[1]
}
which.min(cv.error_poly)
which.min(cv.error_df)
## (f)
library(boot)
set.seed(123)
cv.error_df <- rep(3, 10)
for(i in 3:10) {
df_val <- i
fit <- glm(nox ~ bs(dis, df = df_val), data = Boston)
cv.error_df[i] <- cv.glm(Boston, fit, K = 10)$delta[1]
}
which.min(cv.error_df)
## (f)
library(boot)
set.seed(123)
cv.error_df <- rep(3, 10)
for(i in 3:10) {
df_val <- i
fit <- glm(nox ~ bs(dis, df = df_val), data = Boston)
cv.error_df[i] <- cv.glm(Boston, fit, K = 10)$delta[1]
print(cv.error_df[i])
}
which.min(cv.error_df)
setwd("/Users/jiatao/Desktop/UMich/25Winter/DATASCI415/Homework/HW07")
train <- read.csv("spam-train.txt", header = FALSE)
test <- read.csv("spam-test.txt", header = FALSE)
setwd("/Users/jiatao/Desktop/UMich/25Winter/DATASCI415/Homework/HW07")
train <- read.csv("spam-train.txt", header = FALSE)
test <- read.csv("spam-test.txt", header = FALSE)
View(train)
library(tree)
install.packages("tree")
library(tree)
tree.train = tree(V58~., train)
summary(tree.carseats)
summary(tree.train)
Spam.test= test$V58
tree.pred = predict(tree.train, test ,type="class")
tree.train = tree(V58~., train, type="class")
train$V58 <- as.factor(train$V58)
tree.train = tree(V58~., train)
summary(tree.train)
Spam.test= test$V58
tree.pred = predict(tree.train, test ,type="class")
table(tree.pred,Spam.test)
(854 + 540) / (854 + 78 + 62 + 540)
(78 + 62) / (854 + 78 + 62 + 540)
78 / (78 + 540)
62 / (854 + 62)
set.seed(7)
set.seed(7)
cv.spam = cv.tree(tree.train, FUN=prune.misclass)
names(cv.carseats)
names(cv.spam)
cv.spam
par(mfrow=c(1,2))
plot(cv.spam$size,cv.spam$dev / length(train),ylab="cv error", xlab="size",type="b")
plot(cv.spam$k, cv.spam$dev / length(train),ylab="cv error", xlab="k",type="b")
prune.spam = prune.misclass(tree.train, best=6)
plot(prune.spam)
text(prune.spam,pretty=0)
prune.spam = prune.misclass(tree.train, best=8)
plot(prune.spam)
text(prune.spam,pretty=0)
set.seed(7)
cv.spam = cv.tree(tree.train, FUN=prune.misclass)
names(cv.spam)
cv.spam
par(mfrow=c(1,2))
plot(cv.spam$size,cv.spam$dev / length(train),ylab="cv error", xlab="size",type="b")
plot(cv.spam$k, cv.spam$dev / length(train),ylab="cv error", xlab="k",type="b")
prune.spam = prune.misclass(tree.train, best=8)
plot(prune.spam)
text(prune.spam,pretty=0)
par(mfrow=c(1,1))
plot(prune.spam)
text(prune.spam,pretty=0)
library(randomForest)
install.packages("randomForest")
library(randomForest)
library(randomForest)
rf <- randomForest(
V58~., # Model formula
data=train, # Training data
mtry=ncol(train)-1, # Use all columns
importance=TRUE) # Return feature importance measures
rf
rf_preds <- predict(rf, newdata=test)
plot(
rf_preds,
test$V58,
xlab="Prediction",
ylab="Actual",
main="Median Home Values: Predictions vs. Actuals")
rf_preds
table(rf_preds, Spam.test)
(22 + 34) / (882 + 22 + 34 + 596)
34 / (882 + 34)
34 / (882 + 34)
22 / (596 + 22)
importance(rf)
varImpPlot(rf)
varImpPlot(rf, n.var=10)
setwd("/Users/jiatao/Desktop/UMich/25Winter/DATASCI415/Homework/HW07")
setwd("/Users/jiatao/Desktop/UMich/25Winter/DATASCI415/Homework/HW07")
train <- read.csv("spam-train.txt", header = FALSE)
test <- read.csv("spam-test.txt", header = FALSE)
library(tree)
train$V58 <- as.factor(train$V58)
tree.train = tree(V58~., train)
summary(tree.train)
Spam.test= test$V58
tree.pred = predict(tree.train, test ,type="class")
table(tree.pred,Spam.test)
(78 + 62) / (854 + 78 + 62 + 540)
78 / (78 + 540)
62 / (854 + 62)
set.seed(7)
cv.spam = cv.tree(tree.train, FUN=prune.misclass)
names(cv.spam)
cv.spam
par(mfrow=c(1,2))
plot(cv.spam$size,cv.spam$dev / length(train),ylab="cv error", xlab="size",type="b")
plot(cv.spam$k, cv.spam$dev / length(train),ylab="cv error", xlab="k",type="b")
prune.spam = prune.misclass(tree.train, best=8)
par(mfrow=c(1,1))
plot(prune.spam)
text(prune.spam,pretty=0)
set.seed(7)
cv.spam = cv.tree(tree.train, FUN=prune.misclass)
#names(cv.spam)
#cv.spam
#par(mfrow=c(1,2))
#plot(cv.spam$size,cv.spam$dev / length(train),ylab="cv error", xlab="size",type="b")
#plot(cv.spam$k, cv.spam$dev / length(train),ylab="cv error", xlab="k",type="b")
prune.spam = prune.misclass(tree.train, best=8)
par(mfrow=c(1,1))
plot(prune.spam)
text(prune.spam,pretty=0)
set.seed(7)
cv.spam = cv.tree(tree.train, FUN=prune.misclass)
#names(cv.spam)
#cv.spam
#par(mfrow=c(1,2))
#plot(cv.spam$size,cv.spam$dev / length(train),ylab="cv error", xlab="size",type="b")
#plot(cv.spam$k, cv.spam$dev / length(train),ylab="cv error", xlab="k",type="b")
prune.spam = prune.misclass(tree.train, best=8)
par(mfrow=c(1,1))
plot(prune.spam)
text(prune.spam,pretty=0)
View(train)
library(randomForest)
rf <- randomForest(
V58~., # Model formula
data=train, # Training data
mtry=ncol(train)-1, # Use all columns
importance=TRUE) # Return feature importance measures
rf
rf_preds <- predict(rf, newdata=test)
table(rf_preds, Spam.test)
(22 + 34) / (882 + 22 + 34 + 596)
34 / (882 + 34)
22 / (596 + 22)
importance(rf)
varImpPlot(rf, n.var=10)
library(randomForest)
rf <- randomForest(
V58~., # Model formula
data=train, # Training data
mtry=ncol(train)-1, # Use all columns
importance=TRUE) # Return feature importance measures
rf_preds <- predict(rf, newdata=test)
table(rf_preds, Spam.test)
(22 + 34) / (882 + 22 + 34 + 596)
34 / (882 + 34)
22 / (596 + 22)
library(randomForest)
rf <- randomForest(
V58~., # Model formula
data=train, # Training data
mtry=ncol(train)-1, # Use all columns
importance=TRUE) # Return feature importance measures
rf_preds <- predict(rf, newdata=test)
table(rf_preds, Spam.test)
(20 + 33) / (883 + 20 + 33 + 598)
33 / (883 + 33)
20 / (598 + 20)
importance(rf)
varImpPlot(rf, n.var=10)
importance(rf).head
imp <- as.data.frame(importance(rf))
imp$Variable <- row.names(imp)
imp_sorted <- imp[order(imp$MeanDecreaseGini, decreasing = TRUE), ]
head(imp_sorted, 10)
df <- as.data.frame(importance(rf))
df$Variable <- row.names(df)
df_sorted <- df[order(df$MeanDecreaseGini, decreasing = TRUE), ]
head(df_sorted, 10)
varImpPlot(rf, n.var=10)
